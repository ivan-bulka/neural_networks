{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "# more info on callbakcs: https://keras.io/callbacks/ model saver is cool too.\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import pickle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "sess = tf.Session(config = tf.ConfigProto(gpu_options = gpu_options))\n",
    "\n",
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0731 11:17:08.381842 15292 deprecation.py:506] From C:\\Users\\ivan1\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0731 11:17:08.567734 15292 deprecation.py:323] From C:\\Users\\ivan1\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-conv-32-nodes-1-dense-1564561028\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/2\n",
      "17462/17462 [==============================] - 26s 2ms/sample - loss: 0.6430 - acc: 0.6254 - val_loss: 0.5895 - val_acc: 0.6928\n",
      "Epoch 2/2\n",
      "17462/17462 [==============================] - 24s 1ms/sample - loss: 0.5649 - acc: 0.7122 - val_loss: 0.6331 - val_acc: 0.6400\n",
      "2-conv-32-nodes-1-dense-1564561080\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/2\n",
      "17462/17462 [==============================] - 43s 2ms/sample - loss: 0.6292 - acc: 0.6358 - val_loss: 0.5517 - val_acc: 0.7150\n",
      "Epoch 2/2\n",
      "17462/17462 [==============================] - 44s 3ms/sample - loss: 0.5309 - acc: 0.7371 - val_loss: 0.5024 - val_acc: 0.7588\n",
      "1-conv-64-nodes-1-dense-1564561168\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/2\n",
      "17462/17462 [==============================] - 51s 3ms/sample - loss: 0.6483 - acc: 0.6254 - val_loss: 0.5912 - val_acc: 0.7018 0s - loss: 0.6483 - acc: 0.625\n",
      "Epoch 2/2\n",
      "17462/17462 [==============================] - 51s 3ms/sample - loss: 0.5613 - acc: 0.7156 - val_loss: 0.5412 - val_acc: 0.7298\n",
      "2-conv-64-nodes-1-dense-1564561274\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/2\n",
      "17462/17462 [==============================] - 78s 4ms/sample - loss: 0.6142 - acc: 0.6526 - val_loss: 0.5449 - val_acc: 0.7301\n",
      "Epoch 2/2\n",
      "17462/17462 [==============================] - 76s 4ms/sample - loss: 0.5203 - acc: 0.7448 - val_loss: 0.4876 - val_acc: 0.7670\n",
      "1-conv-32-nodes-2-dense-1564561431\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/2\n",
      "17462/17462 [==============================] - 29s 2ms/sample - loss: 0.6263 - acc: 0.6459 - val_loss: 0.5562 - val_acc: 0.7167TA: 5s - loss: 0.6362 - ac - ETA: 4s - loss: 0.6\n",
      "Epoch 2/2\n",
      "17462/17462 [==============================] - 28s 2ms/sample - loss: 0.5424 - acc: 0.7241 - val_loss: 0.5495 - val_acc: 0.7175\n",
      "2-conv-32-nodes-2-dense-1564561493\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/2\n",
      "17462/17462 [==============================] - 44s 3ms/sample - loss: 0.6609 - acc: 0.5828 - val_loss: 0.5881 - val_acc: 0.68480s - loss: 0.6616 - acc: 0.582 - ETA: 0s - loss: 0.6613 - acc: 0.58\n",
      "Epoch 2/2\n",
      "17462/17462 [==============================] - 46s 3ms/sample - loss: 0.5378 - acc: 0.7291 - val_loss: 0.5205 - val_acc: 0.7500\n",
      "1-conv-64-nodes-2-dense-1564561587\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/2\n",
      "17462/17462 [==============================] - 53s 3ms/sample - loss: 0.6217 - acc: 0.6487 - val_loss: 0.5807 - val_acc: 0.6986: 0.6291 - acc: 0. - ETA: 6s - loss: 0.6287 - acc: 0.639 - ETA: 6s - loss: 0.6285 - acc: 0. - ETA: 6s - loss: 0.6282 - - ETA: 5s - loss: 0 - ETA: 0s - loss: 0.6218 - acc: 0.64\n",
      "Epoch 2/2\n",
      "17462/17462 [==============================] - 51s 3ms/sample - loss: 0.5289 - acc: 0.7424 - val_loss: 0.5218 - val_acc: 0.7417\n",
      "2-conv-64-nodes-2-dense-1564561697\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/2\n",
      "17462/17462 [==============================] - 81s 5ms/sample - loss: 0.6395 - acc: 0.6203 - val_loss: 0.5437 - val_acc: 0.7288\n",
      "Epoch 2/2\n",
      "17462/17462 [==============================] - 80s 5ms/sample - loss: 0.5278 - acc: 0.7360 - val_loss: 0.5255 - val_acc: 0.7420\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [1,2]\n",
    "layer_sizes = [32,64]\n",
    "conv_layers = [1,2]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            tensorboard = TensorBoard(log_dir='logs\\{}'.format(NAME))\n",
    "            print(NAME)\n",
    "\n",
    "\n",
    "            # Model \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            \n",
    "            for l in range(conv_layer-1): \n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "            \n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "                \n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'],\n",
    "                          )\n",
    "\n",
    "            model.fit(X, y,\n",
    "                      batch_size=32,\n",
    "                      epochs=2,  # decreased the number of epocs to make it faster \n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=logs/ --host=127.0.0.1 - for launching tensorboard "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the model with besta parameters. Imagine it's:\n",
    "- dense_layers = [0] \n",
    "- layer_sizes = [64] \n",
    "- conv_layers = [3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-conv-64-nodes-0-dense-1564563690\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/4\n",
      "17462/17462 [==============================] - 81s 5ms/sample - loss: 0.6565 - acc: 0.6017 - val_loss: 0.5846 - val_acc: 0.7040\n",
      "Epoch 2/4\n",
      "17462/17462 [==============================] - 86s 5ms/sample - loss: 0.5654 - acc: 0.7083 - val_loss: 0.5346 - val_acc: 0.7394\n",
      "Epoch 3/4\n",
      "17462/17462 [==============================] - 84s 5ms/sample - loss: 0.5069 - acc: 0.7578 - val_loss: 0.5508 - val_acc: 0.7388\n",
      "Epoch 4/4\n",
      "17462/17462 [==============================] - 81s 5ms/sample - loss: 0.4727 - acc: 0.7774 - val_loss: 0.4927 - val_acc: 0.7682\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [0]\n",
    "layer_sizes = [64]\n",
    "conv_layers = [3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            tensorboard = TensorBoard(log_dir='logs\\{}'.format(NAME))\n",
    "            print(NAME)\n",
    "\n",
    "\n",
    "            # Model \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            \n",
    "            for l in range(conv_layer-1): \n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "            \n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "                \n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'],\n",
    "                          )\n",
    "\n",
    "            model.fit(X, y,\n",
    "                      batch_size=32,\n",
    "                      epochs=4,  # decreased the number of epocs to make it faster \n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('64x3_CNN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3-conv-64-nodes-0-dense-1564565817\n",
      "Train on 17462 samples, validate on 7484 samples\n",
      "Epoch 1/8\n",
      "17462/17462 [==============================] - 84s 5ms/sample - loss: 0.6489 - acc: 0.6094 - val_loss: 0.5855 - val_acc: 0.7006\n",
      "Epoch 2/8\n",
      "17462/17462 [==============================] - 72s 4ms/sample - loss: 0.5483 - acc: 0.7200 - val_loss: 0.5005 - val_acc: 0.7603\n",
      "Epoch 3/8\n",
      "17462/17462 [==============================] - 74s 4ms/sample - loss: 0.4912 - acc: 0.7618 - val_loss: 0.4968 - val_acc: 0.7628\n",
      "Epoch 4/8\n",
      "17462/17462 [==============================] - 72s 4ms/sample - loss: 0.4512 - acc: 0.7887 - val_loss: 0.4558 - val_acc: 0.7887\n",
      "Epoch 5/8\n",
      "17462/17462 [==============================] - 72s 4ms/sample - loss: 0.4126 - acc: 0.8103 - val_loss: 0.4397 - val_acc: 0.7977\n",
      "Epoch 6/8\n",
      "17462/17462 [==============================] - 72s 4ms/sample - loss: 0.3836 - acc: 0.8275 - val_loss: 0.4190 - val_acc: 0.8101\n",
      "Epoch 7/8\n",
      "17462/17462 [==============================] - 72s 4ms/sample - loss: 0.3563 - acc: 0.8408 - val_loss: 0.4138 - val_acc: 0.8140\n",
      "Epoch 8/8\n",
      "17462/17462 [==============================] - 73s 4ms/sample - loss: 0.3333 - acc: 0.8528 - val_loss: 0.4280 - val_acc: 0.8133\n"
     ]
    }
   ],
   "source": [
    "dense_layers = [0]\n",
    "layer_sizes = [64]\n",
    "conv_layers = [3]\n",
    "\n",
    "for dense_layer in dense_layers:\n",
    "    for layer_size in layer_sizes:\n",
    "        for conv_layer in conv_layers:\n",
    "            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(conv_layer, layer_size, dense_layer, int(time.time()))\n",
    "            tensorboard = TensorBoard(log_dir='logs\\{}'.format(NAME))\n",
    "            print(NAME)\n",
    "\n",
    "\n",
    "            # Model \n",
    "            model = Sequential()\n",
    "\n",
    "            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n",
    "            model.add(Activation('relu'))\n",
    "            model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "            \n",
    "            for l in range(conv_layer-1): \n",
    "                model.add(Conv2D(layer_size, (3, 3)))\n",
    "                model.add(Activation('relu'))\n",
    "                model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "            model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "            \n",
    "            for l in range(dense_layer):\n",
    "                model.add(Dense(layer_size))\n",
    "                model.add(Activation('relu'))\n",
    "                \n",
    "            model.add(Dense(1))\n",
    "            model.add(Activation('sigmoid'))\n",
    "\n",
    "            model.compile(loss='binary_crossentropy',\n",
    "                          optimizer='adam',\n",
    "                          metrics=['accuracy'],\n",
    "                          )\n",
    "\n",
    "            model.fit(X, y,\n",
    "                      batch_size=32,\n",
    "                      epochs=8,  # decreased the number of epocs to make it faster \n",
    "                      validation_split=0.3,\n",
    "                      callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('64x3x8ep_CNN.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
